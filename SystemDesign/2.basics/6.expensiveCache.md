### now with the current approach we are seeing redis being costing so high
- lets see what are the queries we are doing
    - Most popular t-shirts
    - recently added
    - Users recently viewed

-  now we have noticed that data for each of these queries have been stored along with the metadata, most popular tshirt i see getting fetched from and then stored in redis to get the latest data every time
-  we can do some optimization well for getting  the most popular t-shirt query on db will be select count(*), tshirt_id from tshirt_views group by tshirt_id order by views, now we can store this data to our redis , but somehow our team is not storing this becuase in redis if someone comes and update the db then we will have stale data , well that is true
-  but this is just most popular tshirt, it is not really going to effect the business even if we show the stale data , so this way we can improve the query
-  recently added or users recently viewed , what we can do in this case we will just store the user_id mapping to tshirt_ids , we wont store the entire metadata, if user is requesting the data for any tshirt we can just query the db and show it to users

- now lets talk about updating the cache , how can we make the cache consistent , well we have few write policy, two are the most popular
    - write through policy (the moment we are upadting we will first update the db and after that we can update the cache as well)
    - write behind policy (in this policy , update comes update the cache and wait , once all the update comes and its time for this key to be evicted at the time this key is getting removed now we will update the db this policy is quiet useful in many situation)
    - write aside policy
    - write asound policy

-  now to implement the user recently viewed cache , for every users we are storing ids of tshirt they viewed , if we have millions of users then we will end up storing millions of arrays
-  how can improve it , well we can improve it , we will basically distribute the data, based on more frequent user (user_id vs num_of_logins) and recently active users (user id vs
time)
- so the table with recently logged in user , now lets a new user logsin and our table with recently logged in has already reached limit then how will we upadate the new user we will follow an eviction policy least recently used which means the user which was on last of table with the oldest login data will be removed and new user will be updated
- and with the table where we are stroing most freqeust users , to add a user here will probably take becasue if to remove we have to use least freqeuntly used eveiction to remove
- so it totally depends on the situation which eviction policy will we use either LRU or LFU

### so the algorithm that determines which entry will be removed from the cache is using LRU and LFU
