### now we have got the requirements for sales day, day when we are expecting 10x user than the usual
- how can we make our system handle these many request
- lets first discuss how many users are we expecting
    - we can ask the founder about number of users and got some information roughly about 20k user on social media and 50k users already registered , and 1 million impressions on ads
- so with that what we can do we increase the server memory and cpu for now , and do some load testing for 10k users then 20k users it worked fine
- on the day of sale , we notice the flood of request coming to the server and our server crashed down
- now we will see why exactly it happened we saw there are 1000 of requests coming from a single user
- which can definetly by DDoS (distributed denial service of attack) could be from the competitors or any one
- request of users are taking so long to response

### Preparing for another sale
### so what should we do in such cases
- Gete-keeping to avoid malicious attacks (cloudflare , aws has some firewall which will stop bots to access the server), from the server side before user can actually access the server, we can have the gateway which will block all such mallicious users
- Graceful degradation incase of over-loaded requests (respond to requests based on priorities)
- Discarding requests altogether if needed (useful incase any downstream or upstream service fails or is re-starting)
    - now requests comes to server and then goes to cache server and then reqeusts to db (if not in cache) becasue of heavy load db is becoming slower latency is going down
    - what can be in this case when db is under heavy load or cache server is under heavy load , we can fail those request in upstream service like server itself, this is called **short circuit** (when upstream service stops sending requests to the downstream service like redis , db)
    - back pressure: when the downstream services start failing requests from the upstream services (this happens when there is huge number of requests coming from upstream to downstream services in this case when downstream services faces heavy load in such situation when server starts recovering then these downstream services will start dropping those upstream reqeusts)
<img width="600" height="400" alt="image" src="https://github.com/user-attachments/assets/a75ea273-c7d5-4875-aa7c-20aa2190e6a6" />

### As of now how is our requests getting processed, it is being processed in queue , FIFO Queue
- what if one of the request takes too long to process then other requests will also get blocked or have to wait for one of the request to get processed, this is called **Head Of Line Blocking**

### Ways to handle head of line blocking
- Parallelism
    - parallel queues and servers
    - multiple virtual machines in the same server
- Concurrency
    - same queue used but multi-thread approach is used to process requests out of order
    - using network protocol which supports having multiple queue , gRPC is an open-source framework for high performance , language-agnostic remote procedure calls using HTTP/2
    - gRPC avoids head of line blocking by processing request out of order

### Now if i dont want to overload the system , for that we can implemente rate limiter (few popular ratelimiters are):
- Token Bucket
- Leaky Bucket

### Two type of rate limiting 
- Loacal Rate limiting
    - Each server decides its own rate limiting
- Centralized rate limiting
