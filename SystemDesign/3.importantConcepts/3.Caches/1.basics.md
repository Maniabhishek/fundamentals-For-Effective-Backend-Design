### Caching
- cache is nothing but in memory storage
- Avoiding repeated work through storage
- basically the idea is to reduce the reponse time , latency of api call, storing large and complex computation in cache for faster response

### How do i manage writes 
- for write there are different write policies

### How do i evict cache
- using eviction policies like LFU, LRU etc
- when our cache is full and lets say new data or video has gone viral and want it to be in cache how will we remove a data (evict a key) in order to insert a new one
- we will different policies to evict data

### what are the disadvantages 
- well lets say there is some data which is not in cache , then it will go to db and return the data , everytime this happens it will add extra latency everytime
- lets say we have a user to query something sequenctially [1,2,3,4,1,2,3,4,1] and we have a storage limit upto 1 to 3, now user come request 1 get from db update cache , same for 2 and then 3 , now when 4 comes -> not founc in cache -> get from db -> save in cache but no space -> we need to evict some key -> so according to LRU 1 will be removed and stored in cache now in cache we have [4,2,3] -> again 1 comes for query we see no data in cache -> goes to db fetches it -> stores in cahce in place of 2 -> 2 is now removed
  - this way we are continuously writing and reading through cache inefficiently such a worse situation is called as thrashing
- eventual consistency , need to add logic for maintaing the consistency from db
